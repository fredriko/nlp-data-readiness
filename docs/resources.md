# Resources

## Data readiness

* [Data Readiness](http://data-readiness.org/)
* [Data Readiness Levels](https://arxiv.org/abs/1705.02245), white paper by Neil Lawrence (2017)
* [FAIR](https://www.go-fair.org/fair-principles/), guidelines to improve the Findability, Accessibility, Interoperability, and Reuse of digital assest (data).


## Resources

Here are links to some common data science/analytics project processes.

* [Cross-industry standard process for data mining (CRISP-DM)](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)
* [Team Data Science Process (TDSP)](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle) by Microsoft.
* [5i Framework](https://medium.com/quantumblack/the-protocol-series-articulating-the-lifecycle-of-an-analytics-use-case-with-the-5i-framework-9959b0306eee) by QuantumBlack.
* [Data Science Project Management](http://www.datascience-pm.com/)


## Text annotation tools

If the solution to the problem involves supervised learning, or assessment of performance based on human labelled data,
then you need to obtain annotated data. There are numerous software tools and services available to accommodate all
sorts of textual annotations. Below is a list of annotation services and tools, as well as a link to a Google spreadsheet with a 
feature comparison matrix of the tools.

* [Brat](https://brat.nlplab.org/)
* [Doccano](https://doccano.herokuapp.com/)
* [Inception](https://inception-project.github.io/)
* [Labelbox](https://labelbox.com/)
* [Label studio](https://labelstud.io/)
* [Light tag](https://www.lighttag.io/)
* [Lionbridge](https://lionbridge.ai/data-annotation-platform/)
* [Prodigy](https://prodi.gy/)
* [SMART](https://github.com/RTIInternational/SMART)
* [Tagtog](https://www.tagtog.net/)
* [YEDDA](https://github.com/jiesutd/YEDDA)

[Feature comparison of the above text annotation tools](https://docs.google.com/spreadsheets/d/1iKO0PhOhthsjZHGyhmsmYKKOxdmdJxrCJdNYevB3L4A/edit?usp=sharing)